---
title: "Hurricanes"
author: "Alexis Scamman"
date: "5/6/2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})

knitr::opts_chunk$set(echo = TRUE, eval = TRUE,fig.align="center",warning=FALSE,message=FALSE,fig.width=8, fig.height=5, linewidth=60)
options(tibble.width = 100,width = 100)
library(tidyverse)
```

## Data

The dataset I chose includes information on named hurricanes in the Atlantic Ocean. I found this dataset interesting becuase it comes from a paper that claimed that hurricanes with female names were not taken as seriously and therefore caused more deaths and damage. The main variables include: Name, wind speed in MPH, atmospheric pressure at landfall in millibars, date of first landfall, property damage in millions of 2014 US dollars, deaths, and whether the hurricane had a male or female name. I created my own categorical variable by splitting the damage costs into two categories - high (more than 1B) and low (less than 100 M). There are 94 observations. 

## MANOVA, ANOVA, Pairwise t-tests, and Type 1 Error

**1. (15 pts)** Perform a MANOVA testing whether any of your numeric variables (or a subset of them, if including them all is unreasonable or doesn't make sense) show a mean difference across levels of one of your categorical variables (3). If they do, perform univariate ANOVAs to find response(s) showing a mean difference across groups (3), and perform post-hoc t tests to find which groups differ (3). Discuss the number of tests you have performed, calculate the probability of at least one type I error (if unadjusted), and adjust the significance level accordingly (bonferroni correction) before discussing significant differences (3). Briefly discuss some of the MANOVA assumptions and whether or not they are likely to have been met here (no need for anything too in-depth) (2).

```{r cars}
#Data
library(readxl)
hurricNamed <- read_csv("hurricNamed.csv")

hurricane <- hurricNamed%>%select(-BaseDamage, -AffectedStates) %>% mutate(damage_level=case_when(BaseDam2014>1000 ~ "high", 1000>BaseDam2014 ~ "low")) %>% separate(firstLF, into=c("year", "month", "day"))
glimpse(hurricane)

#MANOVA
man <- manova(cbind(LF.WindsMPH, LF.PressureMB, LF.times, BaseDam2014, NDAM2014, deaths)~damage_level,data=hurricane)
summary(man)
summary.aov(man)

#ANOVA
summary(aov(LF.WindsMPH~damage_level,data=hurricane))
summary(aov(LF.PressureMB~damage_level,data=hurricane))
summary(aov(LF.times~damage_level,data=hurricane))
summary(aov(BaseDam2014~damage_level,data=hurricane))
summary(aov(NDAM2014~damage_level,data=hurricane))

#pairwise t-tests
pairwise.t.test(hurricane$LF.WindsMPH, hurricane$damage_level, p.adj = "none")
pairwise.t.test(hurricane$LF.PressureMB, hurricane$damage_level, p.adj = "none")
pairwise.t.test(hurricane$LF.times, hurricane$damage_level, p.adj = "none")
pairwise.t.test(hurricane$BaseDam2014, hurricane$damage_level, p.adj = "none")
pairwise.t.test(hurricane$NDAM2014, hurricane$damage_level, p.adj = "none")

#Type I Error
1 - (.95)^21

#Bonferroni adjusted 
.05/21
```
After performing a MANOVA test, 5 of the variables were found to be significant - all of the numeric variables besides number of deaths. Then, a univariate ANOVA test was performed for each of the five significant variables to find the difference across the levels of damage. 21 tests were performed in total. The unadjusted type one error is 0.659, and after adjusting using Bonferroni correction the error is 0.00238.

One MANOVA assumption is that data are random samples and independent observations. Each observation was a different, named hurricane in the Atlantic so they are independent. They might not be a random sample though because it includes every named hurricane between 1950 and 2012. MANOVA also assumes multivariate normality, homogeneity, and linearity- the dependent variables most likely meet these assumption. One last assumption is that there should be no multicollinerity. I removed some variables that had the damage cost at the time versus now because they would be too correlated.


## Randomization Test

**2. (10 pts)** Perform some kind of randomization test on your data (that makes sense). The statistic can be anything you want (mean difference, correlation, F-statistic/ANOVA, chi-squared), etc. State null and alternative hypotheses, perform the test, and interpret the results (7). Create a plot visualizing the null distribution and the test statistic (3).
```{r}
#mean difference for winds
hurricane%>%group_by(damage_level)%>%summarize(means=mean(LF.WindsMPH))%>%summarize("diff_means"=diff(means))

#randomization test for weight
rand_dist<-vector()
for(i in 1:5000){
new<-data.frame(damage=hurricane$damage_level, winds=sample(hurricane$LF.WindsMPH))
rand_dist[i]<-mean(new[new$damage=="high",]$winds)-
              mean(new[new$damage=="low",]$winds)}

mean(rand_dist>18.73016 | rand_dist < -18.73016)

hist(rand_dist,main="",ylab=""); abline(v = c(18.73016,-18.73016),col="red")

#plot
ggplot(new,aes(winds, fill=damage)) + geom_histogram(bins=10) + facet_wrap(~damage, ncol=2) + theme(legend.position = "none")

```
The null hypothesis is that the wind speeds of the hurricane have no affect on the level of damage. The alternative hypothesis is that greater wind speeds will increase the level of damage (high vs low). The p-value is 0.0002 which is p<0.05, so the null hypothesis is rejected.The wind speeds for high damage and low damage differ. The mean difference is -18.73. The randomization test rejects the null hypothesis that wind speed has no effect of the damage level.

## Linear Regression

**3. (40 pts)** Build a linear regression model predicting one of your response variables from at least 2 other variables, including their interaction. Mean-center any numeric variables involved in the interaction.

    - Interpret the coefficient estimates (do not discuss significance) (10)
    - Plot the regression using `ggplot()` using geom_smooth(method="lm"). If your interaction is numeric by numeric,             refer to code in the slides to make the plot or check out the `interactions` package, which makes this easier. If         you have 3 or more predictors, just chose two of them to plot for convenience. (10)
    - What proportion of the variation in the outcome does your model explain? (4)
    - Check assumptions of linearity, normality, and homoskedasticity either graphically or using a hypothesis test (5)
    - Regardless, recompute regression results with robust standard errors via `coeftest(..., vcov=vcovHC(...))`. Discuss         significance of results, including any changes from before/after robust SEs if applicable. (10)

```{r}
#Centering Wind Speed
hurricane2 <- hurricane %>% mutate(cWinds=LF.WindsMPH - mean(LF.WindsMPH,na.rm=T))

#Linear Regression
fit<-lm(BaseDam2014~mf*cWinds, data=hurricane2)
summary(fit)

#Graph
ggplot(hurricane2, aes(y=BaseDam2014, x=cWinds, color=mf)) + geom_point() + geom_smooth(method="lm")

#Checking assumptions
library(lmtest)
library(sandwich)
bptest(fit)

ggplot(fit,aes(cWinds, fill=BaseDam2014)) + geom_histogram(bins=10)

#Regression with robust SE
coeftest(fit,vcov=vcovHC(fit))

```
The intercept is the predicted average cost of damage for female named hurricanes with average wind speeds, which is $4.706 B. The damage cost of male named hurricanes with average wind speeds have $2849.85 M more in damage than female named hurricanes. For every one MPH increase in wind speed, the damage cost of female named hurricanes increases by $65.30 M. For every one MPH increase in wind speed for male named hurricanes, the damage cost increases by $211.37 M. 

The multiple R-squared is 0.09731, and the adjusted R-squared is 0.06722. This is the proportion of the variation of damage amount explained by the model.

The scatter plot shows that there is a linear relationship between the dependent variable, damage cost, and the independent variables, wind speed and male/female name. Normality was checked by eyeing a histogram, and this assumption was not met. homoskedasticity was checked using bptest. The null hypothesis was not rejected because the p-value was 0.78, p>0.05, therefore the data is homoskedastic.

Regression results with robust SEs differs from the results without robust SEs. With robust SEs, the SE of the intercept increased, the SE of the slope of male named hurricanes decreased, the SE of the slope of wind speed increased, and the SE of the slope of wind speed on damage cost for male named hurricanes increased. The p-values also changed with robust SEs. The p-values decreased for the intercept and all slopes except the slope for male named hurricanes, which decreased.

# Bootstrapped Standard Errors for Linear Regression

**4. (5 pts)** Rerun same regression model (with the interaction), but this time compute bootstrapped standard errors (either by resampling observations or residuals). Discuss any changes you observe in SEs and p-values using these SEs compared to the original SEs and the robust SEs)
```{r}

#Bootstrapping by resampling residuals
fit <- lm(BaseDam2014~mf*cWinds, data=hurricane2)
  resids<-fit$residuals
  fitted<-fit$fitted.values
  resid_resamp<-replicate(5000,{
    new_resids<-sample(resids,replace=TRUE)
    hurricane2$new_y<-fitted+new_resids
    fit<-lm(new_y~cWinds+mf,data=hurricane2)
    coef(fit) 
})

resid_resamp%>%t%>%as.data.frame%>%summarize_all(sd)  

```

Compared to the original SE's the bootstrapped SE's are lower for wind speed and a male gender name. Compared to the robust SE's, the bootstrapped SE's are lower for wind speed, but higher for male gender name.

## Logistic Regression

**5. (30 pts)** Fit a logistic regression model predicting a binary variable (if you don't have one, make/get one) from at least two explanatory variables (interaction not necessary).

    - Interpret coefficient estimates in context (10)
    - Report a confusion matrix for your logistic regression (5)
    - Compute and discuss the Accuracy, Sensitivity (TPR), Specificity (TNR), Precision (PPV), and AUC of your model (5)
    - Using ggplot, make a density plot of the log-odds (logit) colored/grouped by your binary outcome variable (5)
    - Generate an ROC curve (plot) and calculate AUC (either manually or with a package); interpret (5)
```{r}

library(tidyverse)
library(lmtest)

#Dummy code
hurricane2<-hurricane2%>%mutate(y=ifelse(damage_level=="high",1,0))
head(hurricane2)

#Logistic Regression
logfit<-glm(y~cWinds + mf, data=hurricane2, family="binomial")
coeftest(logfit)

#odds of high damage level
1*10^-0.089322
#increase in odds of high damage level from wind speed
1*10^0.047487
#increase in odds of high damage level from male name
1*10^0.088070

#Confusion Matrix
prob <- predict(logfit, type="response")
table(predict = as.numeric(prob > .5), truth = hurricane2$y) %>% addmargins

#accuracy
(36+32)/94
#sensitivity
36/49
#specificity
32/45
#precision
36/49

#AUC and ROC plot
library(plotROC)
ROCplot<-ggplot(hurricane2)+geom_roc(aes(d=damage_level,m=prob), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

#Density plot
hurricane2$logit<-predict(logfit,type="link")

hurricane2%>%ggplot()+geom_density(aes(logit,color=damage_level,fill=damage_level), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("logit (log-odds)")+
  geom_rug(aes(logit,color=damage_level))

```
For female named hurricane and wind speed = 0, the log-odds is -0.089, the odds of high damage level is e^-0.089=0.814. When the wind speed increases by one MPH, the odds are multiplied by e^0.0475=1.1155. The odds of high damage level increase by 11.55%. When the hurricane has a male name the odds are multiplied by e^0.0881=1.2248. The odds of a high damage leel increase by 22.48%.

The accuracy is the proportion classified as correct, which was 0.723. The sensitivity is the proportion of high damage level classified correctly, which was 0.734. The specificity is the proportion of low damage level classified as correct, which was 0.711. The precision is propotion of high damage levels that were, which was 0.735. 

An ROC plot was generated, using actual damage level and predicted damage level. The ROC graph shows that wind speed and gender of name were bad predictors of damage level. Normally, if there is are good predictors, it curves the other direction toward the top left corner rather than the bottom right corner. The AUC was calculated to be 0.2678. This is a very bad AUC. The graph looks this way because the AUC is extremely low, therefore very bad.

## Logistic Regression with all variables

**6. (25 pts)** Perform a logistic regression predicting the same binary response variable from *ALL* of the rest of your variables (the more, the better!) 

    - Fit model, compute in-sample classification diagnostics (Accuracy, Sensitivity, Specificity, Precision, AUC), and interpret (5)
    - Perform 10-fold (or repeated random sub-sampling) CV with the same model and report average out-of-sample classification diagnostics (Accuracy, Sensitivity, Specificity, Precision, and AUC); interpret AUC and compare with the in-sample metrics (10)
    - Perform LASSO on the same model/variables. Choose lambda to give the simplest model whose accuracy is near that of the best (i.e., `lambda.1se`). Discuss which variables are retained. (5)
    - Perform 10-fold CV using only the variables lasso selected: compare model's out-of-sample AUC to that of your logistic regressions above (5)
    
```{r}
class_diag<-function(probs,truth){
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]
  
  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

class_diag(prob,hurricane2$y)

#k-fold CV
hurricane3<-hurricane2%>%select(-Hurricane,-Name,-Year,-year,-day,-month,-damage_level,-logit,-BaseDam2014,-NDAM2014)%>%na.omit #I had to keep removing variables because running it with these caused an error code, but when I removed them all it ran

set.seed(1234)
k=10
data<-hurricane3[sample(nrow(hurricane3)),]
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F)

diags<-NULL

for(i in 1:k){
  
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$y
  
  fit<-glm(y~.,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth))
}

summarize_all(diags,mean) #average diagnostics across all k folds



# LASSO
library(glmnet)
set.seed(1234)
y <- as.matrix(hurricane3$y) 
x <- model.matrix(y~., data = hurricane3)[,-1]
x<- scale(x)
cv <- cv.glmnet(x, y, family = "binomial")
lasso <- glmnet(x, y, family = "binomial", lambda = cv$lambda.1se)
coef(lasso)

#k-fold with LASSO variables
lasso_data <- hurricane3%>%select(LF.PressureMB, LF.times, deaths, mf, y)

set.seed(1234)
k=10
data <- lasso_data[sample(nrow(lasso_data)),]
folds <- cut(seq(1:nrow(lasso_data)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$y 
  fit<-glm(y~(.)^2, data=lasso_data, family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  diags<-rbind(diags,class_diag(probs,truth))
}
summarize_all(diags,mean)
```
I ran class_diag on hurricane 2 datatset: the accuracy is 0.7234043, the sensitivity is 0.7111111, the specificity is 0.7346939, the precision is 0.7111111, and the AUC is 0.7321995. The AUC shows the variables are a fair predictor of damage level.

A k-fold CV was attempted to be performed on the logistic regression to predict damage level using all of the variables in hurricane2. However, several error codes were received so I removed many of the categorical variables, such as hurricane, name, year, month, day, and saved as a new dataset, hurricane3. I also removed NDAM2014 and BaseDam2014 because they were used to make the binomial variable. After this, the k-fold test was performed. The new acc was 0.925, sens was 0.841, spec was 0.842, ppv was 0.832, and auc was 0.925, which is extremely high, therefore, a great predictor.  The class_diag gave back only "fair" results. The out of sample AUC actually increased compared to the original AUC.

When LASSO was run, atmospheric pressure, land fall times, deaths, and Winds (centered) was retained. (Side note: The gender of name of the hurricane was not retained, which I find interesting because the claim of the paper where this data came from was that hurricanes with female names were taken less seriously and therefore caused more damage.) I ran another k-fold CV with only the variables retained from LASSO. The acc was 0.84, sens was 0.818, spec was 0.867, ppv was 0.85, and auc was 0.938. The AUC increased, which makes sense because LASSO retains the variables that are the strongest predictors.


```{r pressure, echo=FALSE}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
